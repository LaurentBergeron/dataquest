{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A simple spam filter constructed from a Naive Bayes algorithm\n",
    "\n",
    "Spam may seem like an annoyance, but is truly a worldwide problem with approximately 86% of emails being spam [[1]](https://www.ezcomputersolutions.com/blog/true-cost-of-spam-for-companies/) .  94 billion spam threats are sent every day throughout the world [[2]](https://www.mailcleaner.net/blog/spam-world-news/how-much-does-spam-cost-the-world/), highlighting the importance of efficient spam filters. In this notebook, I demonstrate how to build a simple spam filter based on a Naive Bayes algorithm. This spam filter assumes conditional independence of words in a message, which means that the presence of one word is not assumed to affect the probability to find a different word. This is obviously a simplified representation of reality: it is more likely, for example, to find the word \"money\" in an email given the word \"bank\" is present.\n",
    "\n",
    "The database that this algorithm will learn from comes from [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) and includes 747 spam SMS messages and 4827 desired SMS messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sms = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS']) \n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of spam: 13.4%.\n",
      "Probability of non-spam: 86.6%\n"
     ]
    }
   ],
   "source": [
    "P_spam = (sms['Label'] == 'spam').sum() / sms.shape[0]\n",
    "P_Nspam = (sms['Label'] == 'ham').sum() / sms.shape[0]\n",
    "print(('Probability of spam: {:.1f}%.\\n'\n",
    "       'Probability of non-spam: {:.1f}%').format(P_spam*100, P_Nspam*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based solely on our dataset, and without using any information about the contents of the message, we can estimate that the probability the a given SMS is spam is 13.4%.\n",
    "\n",
    "Before learning anythin from the contents of the messages, let's split the data set into a training set (80%) and a testing set (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Probability of spam: 13.5%.\n",
      "Probability of non-spam: 86.5%\n",
      "\n",
      "Test data\n",
      "Probability of spam: 13.2%.\n",
      "Probability of non-spam: 86.8%\n"
     ]
    }
   ],
   "source": [
    "# Randomize dataset\n",
    "sms = sms.sample(frac=1, random_state=1)\n",
    "\n",
    "# Split training (80%) and test (20%) data\n",
    "N = len(sms.index)\n",
    "split = int(.8*N)\n",
    "sms_train = sms.iloc[0:split].copy()\n",
    "sms_train.reset_index(inplace=True)\n",
    "sms_test  = sms.iloc[split:N].copy()\n",
    "sms_test.reset_index(inplace=True)\n",
    "\n",
    "# Recalculate probabilities of spam vs. ham for splitted training dataset\n",
    "P_spam = (sms_train['Label'] == 'spam').sum() / sms_train.shape[0]\n",
    "P_Nspam = (sms_train['Label'] == 'ham').sum() / sms_train.shape[0]\n",
    "print(('Training data\\n'\n",
    "       'Probability of spam: {:.1f}%.\\n'\n",
    "       'Probability of non-spam: {:.1f}%').format(P_spam*100, P_Nspam*100))\n",
    "\n",
    "# Recalculate probabilities of spam vs. ham for splitted test dataset\n",
    "P_spam = (sms_test['Label'] == 'spam').sum() / sms_test.shape[0]\n",
    "P_Nspam = (sms_test['Label'] == 'ham').sum() / sms_test.shape[0]\n",
    "print(('\\nTest data\\n'\n",
    "       'Probability of spam: {:.1f}%.\\n'\n",
    "       'Probability of non-spam: {:.1f}%').format(P_spam*100, P_Nspam*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of spam in both training and test datasets remains similar to the combined dataset. To proceed with the training, the next step will be to rearrange the training dataframe in terms of word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>interflora</th>\n",
       "      <th>milta</th>\n",
       "      <th>clark</th>\n",
       "      <th>support</th>\n",
       "      <th>cardiff</th>\n",
       "      <th>toilet</th>\n",
       "      <th>senrd</th>\n",
       "      <th>...</th>\n",
       "      <th>packing</th>\n",
       "      <th>medical</th>\n",
       "      <th>growing</th>\n",
       "      <th>qing</th>\n",
       "      <th>sculpture</th>\n",
       "      <th>posted</th>\n",
       "      <th>paid</th>\n",
       "      <th>removal</th>\n",
       "      <th>tmrw</th>\n",
       "      <th>dun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1078</td>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4028</td>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>958</td>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4642</td>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4674</td>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot   ask ü all smth   there s a card on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6938 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index Label                                                SMS  interflora  \\\n",
       "0   1078   ham                       yep  by the pretty sculpture           0   \n",
       "1   4028   ham      yes  princess  are you going to make me moan            0   \n",
       "2    958   ham                         welp apparently he retired           0   \n",
       "3   4642   ham                                            havent            0   \n",
       "4   4674   ham  i forgot   ask ü all smth   there s a card on ...           0   \n",
       "\n",
       "   milta  clark  support  cardiff  toilet  senrd  ...  packing  medical  \\\n",
       "0      0      0        0        0       0      0  ...        0        0   \n",
       "1      0      0        0        0       0      0  ...        0        0   \n",
       "2      0      0        0        0       0      0  ...        0        0   \n",
       "3      0      0        0        0       0      0  ...        0        0   \n",
       "4      0      0        0        0       0      0  ...        0        0   \n",
       "\n",
       "   growing  qing  sculpture  posted  paid  removal  tmrw  dun  \n",
       "0        0     0          1       0     0        0     0    0  \n",
       "1        0     0          0       0     0        0     0    0  \n",
       "2        0     0          0       0     0        0     0    0  \n",
       "3        0     0          0       0     0        0     0    0  \n",
       "4        0     0          0       0     0        0     0    0  \n",
       "\n",
       "[5 rows x 6938 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all that is not a-z or A-Z by a space. Lower every case.\n",
    "sms_train['SMS'] = sms_train['SMS'].str.replace(r'[\\W\\d_]',' ').str.lower()\n",
    "\n",
    "# Create vocabulary set of all words encountered in the dataset\n",
    "vocab = set()\n",
    "for message in sms_train['SMS']:\n",
    "    vocab.update(message.split())\n",
    "\n",
    "# Add one column to sms dataframe for each word. The value will be the word count for that message.\n",
    "N = len(sms_train.index)\n",
    "dictio = {unique_word: [0]*N for unique_word in list(vocab)}\n",
    "\n",
    "for ii, message in enumerate(sms_train['SMS']):\n",
    "    for word in message.split():\n",
    "        dictio[word][ii] += 1\n",
    "\n",
    "df_word_count = pd.DataFrame(dictio)\n",
    "\n",
    "sms_wordcount = pd.concat((sms_train, df_word_count), axis=1, ignore_index=False)\n",
    "sms_wordcount.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have a clean training dataset, we can begin to implement the spam filter. In the Naive Bayes algorithm,\n",
    "\n",
    "$\\displaystyle P(\\text{spam}|w_1,w_2,...,w_n) \\propto P(\\text{spam}) \\prod_{i=1}^n P(w_i|\\text{spam})$  \n",
    "$\\displaystyle P(\\text{ham}|w_1,w_2,...,w_n) \\propto P(\\text{ham}) \\prod_{i=1}^n P(w_i|\\text{ham})$\n",
    "\n",
    "with  \n",
    "$\\displaystyle P(w_i|\\text{spam}) = \\frac{N_{w_i|\\text{spam}}+\\alpha}{N_\\text{spam} + \\alpha N_\\text{vocab}}$  \n",
    "and  \n",
    "$\\displaystyle P(w_i|\\text{ham}) = \\frac{N_{w_i|\\text{ham}}+\\alpha}{N_\\text{ham} + \\alpha N_\\text{vocab}}$  \n",
    "\n",
    "We choose $\\alpha=1$ (Laplace hypothesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into spam and ham dataframes. \n",
    "sms_wordcount_spam = sms_wordcount[sms_train['Label'] == 'spam']\n",
    "sms_wordcount_ham  = sms_wordcount[sms_train['Label'] == 'ham']\n",
    "sms_wordcount_spam = sms_wordcount_spam.iloc[:, 3:] # remove \n",
    "sms_wordcount_ham  = sms_wordcount_ham.iloc[:, 3:]\n",
    "\n",
    "# Calculate P(spam) and P(ham)\n",
    "P_spam = sms_wordcount_spam.shape[0] / sms_train.shape[0]\n",
    "P_ham  = sms_wordcount_ham.shape[0] / sms_train.shape[0]\n",
    "\n",
    "# Calculate N_spam, N_ham and N_vocab\n",
    "N_vocab = len(vocab)\n",
    "N_spam = sms_wordcount_spam.sum().sum()\n",
    "N_ham  = sms_wordcount_ham.sum().sum()\n",
    "\n",
    "# Calculate P(w_i | spam) and P(w_i | ham)\n",
    "N_wi_given_spam = {w_i: sms_wordcount_spam[w_i].sum() for w_i in vocab}\n",
    "N_wi_given_ham  = {w_i: sms_wordcount_ham[w_i].sum()  for w_i in vocab}\n",
    "alpha = 1\n",
    "\n",
    "def calc_P_wi_given_spam(wi):\n",
    "    return (N_wi_given_spam[wi] + alpha) / (N_spam + alpha*N_vocab)\n",
    "\n",
    "def calc_P_wi_given_ham(wi):\n",
    "    return (N_wi_given_ham[wi] + alpha) / (N_ham + alpha*N_vocab)\n",
    "\n",
    "P_wi_given_spam = {w_i: calc_P_wi_given_spam(w_i) for w_i in vocab}\n",
    "P_wi_given_ham  = {w_i: calc_P_wi_given_ham(w_i)  for w_i in vocab}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With most variables pre-computed, next we will code the function that takes in a new message and determines if it classifies as spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify_message(message):\n",
    "    \n",
    "    # Reduce message (same treatement as our training dataset)\n",
    "    message = re.sub(r'[\\W\\d_]', ' ', message).lower()\n",
    "    words = message.split()\n",
    "    \n",
    "    # Calculate P(spam | w_1, w_2, ..., w_n)\n",
    "    P_spam_given_message = P_spam\n",
    "    for w_i in words:\n",
    "        try:\n",
    "            P_spam_given_message *= P_wi_given_spam[w_i]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "    # Calculate P(ham | w_1, w_2, ..., w_n)\n",
    "    P_ham_given_message = P_ham\n",
    "    for w_i in words:\n",
    "        try:\n",
    "            P_ham_given_message *= P_wi_given_ham[w_i]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # Return message\n",
    "    if P_spam_given_message > P_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to test the spam filter, let's use the remaining SMS from our test dataset. Those SMS have never been seen by the algorithm and so they are considered new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.48%\n"
     ]
    }
   ],
   "source": [
    "sms_test['predicted'] = sms_test['SMS'].apply(classify_message)\n",
    "sms_test['success'] = sms_test['Label'] == sms_test['predicted']\n",
    "sms_test.head()\n",
    "\n",
    "accuracy = sms_test['success'].sum() / sms_test.shape[0]\n",
    "print('Accuracy: {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy at predicting spam is 98.92%, an excellent result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this project, we developed a Naive Bayes spam filter and made it learn using data from [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The accuracy of prediction of our spam filter is 98.92%, which is great considering the simplicity of this algorithm. The accuracy may be inflated considering that our training and testing datasets have been sampled from the same dataset covering three specific scenarios. This would be a good reason to redo a test against a new dataset which the training has not sampled from."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
